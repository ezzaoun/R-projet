---
title: "Projet_commun"
output: pdf_document
date: "2024-11-04"
---

```{css,echo=F}
.badCode {
background-color: #cfdefc; 
}

.corrO { background-color: rgb(255,238,237); }
.corrS { background-color: pink; color: black; border: 1px solid red; }
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE,
               cache=FALSE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE,
               class.source="badCode")
```

```{r,warning=F,message=F}
library(ggplot2)
library(gridExtra)
library(reshape2)
library(FactoMineR)
library(factoextra)
library(corrplot)
library(forcats)
library(mclust)
library(cluster)
library(ppclust)
library(ggalluvial)
library(klaR)
library(gridExtra)
library(reshape2)
library(clusterSim)
```

## Partie 1 - Analyse descriptivre du jeu de données

## Statistique descriptive

--- 
Le jeu de données étudié possède 1615 génes et 36 variables qualitatives associées.
Nous allons dans un premier temps afficher ce jeu de données.
---
```{r}
Data = read.table("DataProjet.txt", header =T)
knitr::kable(Data)
```

---
Nous allons ensuite afficher la distribution des mesures d'expression des génes pour les différents traitement, réplicat et temps.
---
```{r}
options(max.print = 1000)
summary(Data)
```

```{r}
boxplot(Data)
```

---
On trace ensuite les corrélations entre les différentes variables
---

```{r}
corrplot(cor(Data), method = "ellipse")
# test
```

---
On remarque un certain cadrillage qui se forme.
Pour étudier pour en détaille ce graphique, on va extraire plusieurs partie de ce dernier et expliquer chacune d'entre elles :
 - Extraction des corrélations liées à un traitement et une réplicat : Cette extraction permet de comparer la correlation entre les différentes heures. On prend l'exemple ici de T1 et R1.
 - Extraction des corrélations liées aux même traitement : à compléter
---

```{r}
# Première extraction
corrplot(cor(Data[,1:6]), method = "ellipse")
```

---
On remarque que plus les temps sont proches, plus les variables sont corrélées. Ce résultat s'explique par l'évolution temporelle d'une expérience. Plus le traitement agit longtemps sur le gène, plus le traitement aura un effet important sur le gène.
On prend ici l'exemple de T1 et R1, cependant, on obtient le même résultat sur les autres traitement et les autres réplicats.
---

```{r}
# Deuxième extraction
corrplot(cor(Data)[c(1:6, 19:24), c(1:6, 19:24)], method = "ellipse") # pour le traitement 1
corrplot(cor(Data[c(7:12, 25:30), c(7:12, 25:30)]), method = "ellipse") # pour le traitement 2
corrplot(cor(Data[c(13:18, 31:36), c(13:18, 31:36)]), method = "ellipse") # pour le traitement 3

# Corrélation entre les variables subissant le traitement T1 pour le réplicat R1 et le réplicat R2
corrplot(cor(Data)[1:18, 19:36],method = "ellipse")
corrplot(cor(Data)[19:36, 1:18], method = "ellipse")
```

---
On remarque qu'entre les différents réplicats, les variables ayant le même traitement et la même heure, sont plutôt corrélé entre eux. 
On peut donc remarqué une corrélation importante entre les deux réplicat. Ainsi, une expérience produite deux fois renvoit des résultats qui sont corrélés entre eux.
On remarque cependant que le traitement 1 a des résultats moins corrélé entre la première et la deuxième expérience que le traitement 2 et 3. 
Pour le traitement 2, on remarque de les résultats sont plus corrélé à la fin de l'expérience (quasi égale à 1) que au début, cela signifie que les deux expériences amène au résultat mais ont une vitesse de réaction au traitement différents.
Pour le traitement 3, on remarque que le corrélation sont toutes très proches de 1, cela signifie que les 2 expriences ont une vitesse égale et donc que le traitement à une probalité d'efficaté plus important que les deux précédents.
---
```{r}
ggplot(melt(Data), aes(x = variable, y = value)) + geom_boxplot() + labs(title = "Boxplot des données mises à l'échelle", x = "Variable", y = "Valeur (échelle standardisée)") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
#c'est pas assez informatif
#pour avoir une meilleure representation, on fait un violin plot
```
D’abord, on observe une distinction nette entre les réplicats R1 et R2, témoignant d’une périodicité dans les valeurs d'expression. Ce phénomène s'explique par le fait que les mêmes gènes sont soumis aux mêmes traitements et conditions pour chaque réplicat, bien que des variations aléatoires introduisent de légères différences.

Les boxplots révèlent trois groupes distincts correspondant aux traitements T1, T2, et T3. Pour le traitement T1, les médianes sont proches de 0, indiquant une faible réaction des gènes. En revanche, sous le traitement T2, les médianes montrent une tendance à s’éloigner de 0, suggérant une réponse progressive des gènes. Le traitement T3, combinaison de T1 et T2, montre des profils similaires à T2, ce qui laisse penser que T2 domine dans l’effet global du mélange et induit une réaction notable.

Ces observations s’appliquent de manière presque symétrique aux deux réplicats, illustrant leur homogénéité générale.

## Analyse des variables T_t s_H R_r
```{r}
# transformation du tableau 
transData = t(Data)
knitr::kable(transData)
```

```{r}
# ACP sur les variables : transposée du tableau
respca<-PCA(transData, scale.unit = T, graph=F)

fviz_pca_ind(respca, 
             #geom = "point", 
             col.ind = "cos2", # Coloration selon la qualité de représentation
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE,
             title = "Projection des gènes dans l'espace des composantes principales")

fviz_pca_var(respca, 
             col.var = "contrib", 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             title = "Projection des variables dans l'espace des composantes principales")

fviz_eig(respca)
```


```{r}
reskmeans<-kmeans(transData,centers = 3)
fviz_cluster(reskmeans,transData,ellipse.type = "norm")
fviz_pca_ind(respca,col.ind=as.factor(reskmeans$cluster))
```
on peut voir une séparation en 3 classes distinguées
En analysant ces classes, on peut voir qu'une premiere classe contient les tests qu'on a fait avec T1 sur R1 et R2, ainsi que certains tests avec T2 et T3 à des heures avancées (
```{r}
# courbe du coude
set.seed(1234)
Kmax<-10
reskmeanscl<-matrix(0,nrow=nrow(transData),ncol=Kmax-1)
Iintra<-NULL
for (k in 2:Kmax){
  resaux<-kmeans(transData,centers=k)
  reskmeanscl[,k-1]<-resaux$cluster
  Iintra<-c(Iintra,resaux$tot.withinss)
}

df<-data.frame(K=2:10,Iintra=Iintra)
ggplot(df,aes(x=K,y=Iintra))+geom_line()+geom_point()+xlab("Nombre de classes")+ylab("Inertie intraclasse")
# on peut voir qu'on a entre 3 ou 4 classes, ce qui est attendu vu le graphe de projection des individus de l'ACP
```


```{r}
# Silouhette
Silhou<-NULL
for (k in 2:Kmax){
   aux<-silhouette(reskmeanscl[,k-1], dist(transData))
   Silhou<-c(Silhou,mean(aux[,3]))
}

df<-data.frame(K=2:Kmax,Silhouette=Silhou)
ggplot(df,aes(x=K,y=Silhouette))+
  geom_point()+
  geom_line()+theme(legend.position = "bottom")

aux<-silhouette(reskmeanscl[,2],dist(transData))
fviz_silhouette(aux)+theme(plot.title = element_text(size =9))
rm(df,Silhou,aux)
```


```{r}
hward<-hclust(dist(transData),method = "ward.D2")
CH<-NULL
Kmax<-20
for (k in 2:Kmax){
  CH<-c(CH,index.G1(transData,cutree(hward,k)))
}
daux<-data.frame(NbClust=2:Kmax,CH=CH)
ggplot(daux,aes(x=NbClust,y=CH))+geom_line()+geom_point()

ClustCH<-cutree(hward,k=2)
fviz_dend(hward,k=2,show_labels = FALSE,rect = TRUE, rect_fill = TRUE,palette = "npg",rect_border = "npg",
labels_track_height = 0.8)+ggtitle("")
fviz_pca_ind(respca,geom=c("point"),col.ind=as.factor(ClustCH))
```

)
## Analyse des individus Gènes
# Partie ACP des individus
```{r}
# Exécution de l'ACP sur les individus (les gènes)
pca_result_ind <- PCA(Data, scale.unit = FALSE, graph = FALSE)  # ACP sans mise à l'échelle

# Visualisation des individus (gènes) en fonction du cos2 pour évaluer la qualité de leur projection
fviz_pca_ind(pca_result_ind, col.ind = "cos2", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))

# Afficher les valeurs cos2 des individus pour analyser leur qualité de représentation
pca_result_ind$var$cos2

# Visualisation de l'inertie des axes principaux pour observer la variance expliquée
fviz_eig(pca_result_ind)
```

---
Le premier aspect qui se démarque est la formation de deux clusters distincts. Cela suggère que les gènes se regroupent en fonction de leur expression similaire sous les différentes conditions. Les points proches les uns des autres représentent des gènes ayant des profils d'expression similaires, tandis que les points éloignés indiquent des gènes présentant des différences notables dans leur expression. La coloration des points en fonction du cos2 permet de visualiser la qualité de la projection des gènes dans cet espace réduit, avec des couleurs chaudes signalant une meilleure représentation et une contribution plus forte à l'explication de la variance dans l'espace de faible dimension.
---

# Clustering sur les individus : K-means

```{r}
# Évolution de l'inertie intraclasse pour choisir le nombre optimal de clusters
Kmax <- 15
reskmeanscl <- matrix(0, nrow = nrow(Data), ncol = Kmax - 1)
Iintra <- NULL

# Calcul de l'inertie pour chaque nombre de clusters
for (k in 1:Kmax) {
  resaux <- kmeans(Data, k, nstart = 1)
  reskmeanscl[, k - 1] <- resaux$cluster
  Iintra <- c(Iintra, resaux$tot.withinss)
}

# Affichage de l’évolution de l’inertie en fonction du nombre de clusters
df <- data.frame(K = 1:15, Iintra = Iintra)
ggplot(df, aes(x = K, y = Iintra)) +
  geom_line() +
  geom_point() +
  xlab("Nombre de clusters") +
  ylab("Inertie intraclasse")

```
---
L'inertie intra-classe diminue à mesure que le nombre de clusters augmente. Cependant, lorsque le nombre de clusters atteint une certaine valeur, l'inertie devient stable et ne diminue plus de manière significative. Le point de coude dans le graphique nous aide à identifier le nombre optimal de clusters. Dans ce cas, le coude se forme lorsque le nombre de clusters est égal à 2, ce qui suggère que 2 clusters représentent le meilleur compromis entre la compacité des groupes et la simplicité du modèle.
---

```{r}
# Le critère silhouette
Silhou <- NULL
for (k in 2:Kmax) {
  aux <- silhouette(kmeans(Data, k)$cluster, daisy(Data))
  Silhou <- c(Silhou, mean(aux[, 3]))
}

# Affichage du critère silhouette pour chaque nombre de clusters
df <- data.frame(K = 2:Kmax, Silhouette = Silhou)
ggplot(df, aes(x = K, y = Silhouette)) +
  geom_point() +
  geom_line() +
  theme(legend.position = "bottom")

# Identifier le nombre optimal de clusters basé sur la silhouette
which.max(Silhou)

# Visualisation de la silhouette pour le nombre optimal de clusters
aux <- silhouette(kmeans(Data, centers = which.max(Silhou) + 1)$cluster, daisy(Data))
fviz_silhouette(aux) + theme(plot.title = element_text(size = 9))

```
---
Le critère silhouette évalue la qualité du clustering. La silhouette est élevée, ce qui signifie que les points sont bien regroupés et éloignés des autres groupes. L’analyse permet de confirmer que le nombre optimal de clusters est 2.
---
```{r,eval=F}
reskmeans<-kmeans(Data, 2)  
reskmeans$cluster
fviz_cluster(reskmeans, data=Data,ellipse=F,geom=c("point"))
```

```{r,eval=F}
fviz_pca_ind(pca_result_ind, col.ind=as.factor(reskmeans$cluster), geom="point")
```


# Clustering sur les individus : DBSCAN
```{r,eval=F}
# Grid de recherche pour le paramètre eps et minPts
minPts <- seq(5, 15, 1)
eps <- seq(0.5, 10, 0.3)
NBCluster <- matrix(0, nrow = length(minPts), ncol = length(eps))
NBNonCl <- matrix(0, nrow = length(minPts), ncol = length(eps))

# Recherche de l'optimal eps et minPts
for (i in 1:length(minPts)) {
  for (j in 1:length(eps)) {
    res <- dbscan::dbscan(Data, eps = eps[j], minPts = minPts[i])
    NBCluster[i, j] <- length(table(res$cluster)) - 1
    NBNonCl[i, j] <- sum(res$cluster == 0)
  }
}

# Visualisation des résultats DBSCAN
df <- data.frame(eps = rep(eps, each = length(minPts)),
                 minPts = as.factor(rep(minPts, length(eps))),
                 NBCluster = c(NBCluster),
                 NBNonCl = c(NBNonCl) * 100 / nrow(Data))

# Affichage des clusters formés en fonction de eps et minPts
ggplot(df, aes(x = eps, y = NBCluster, col = minPts)) + 
  geom_point() + 
  geom_line()

# Affichage des points non-clustérisés (bruit)
ggplot(df, aes(x = eps, y = NBNonCl, col = minPts)) + 
  geom_point() + 
  geom_line()

```
---
Les courbes obtenues pour l'analyse DBSCAN montrent l'impact des paramètres eps (distance maximale) et minPts (nombre minimum de voisins) sur la formation des clusters et l'identification du bruit. Les courbes montrent que l'augmentation de eps réduit le nombre de clusters et de points de bruit, tandis que l'augmentation de minPts rend la formation des clusters plus stricte, réduisant aussi les clusters détectés. Les graphiques aident à ajuster ces paramètres pour optimiser le nombre de clusters et limiter le bruit.
---

```{r,eval=F}
# A COMPLETER
# k représente le nombre de variables dans les données
k <- ncol(Data)
k

# Calcul des distances aux k plus proches voisins
dbscan::kNNdistplot(Data, k)
abline(h = 8.5)  # Ligne horizontale pour identifier l'eps optimal (coude)

```
---
Le "coude" de la courbe nous indique un bon choix pour le paramètre eps (ici, eps = 8.5). Cette méthode consiste à calculer la moyenne des distances des k plus proches voisins pour chaque point. Ensuite, nous ordonnons ces distances de manière croissante pour identifier le point d'inflexion (le "coude") qui détermine le meilleur epsilon pour DBSCAN.
---
```{r,eval=F}
# A COMPLETER
res.db = dbscan::dbscan(Data, minPts = 8, eps = 8.5)
res.db
fviz_cluster(res.db, Data, geom="point",ellipse="FALSE")+
  theme(legend.position="none")+
  xlab("")+ylab("")+ggtitle("Avec DBSCAN")

table(res.db$cluster)
```
---
Les clusters obtenus avec l'algorithme DBSCAN sont globalement similaires à ceux générés par K-means, à l'exception de 13 gènes qui ne sont pas classifiés par DBSCAN. Ces gènes sont probablement considérés comme du bruit ou isolés, car DBSCAN ne les associe à aucun cluster en raison de leur éloignement ou de la densité insuffisante des voisins à proximité.
---
```{r,eval=F}
fviz_pca_ind(pca_result_ind, col.ind=as.factor(res.db$cluster), geom="point")
```

```{r}
# Comparaison des résultats de K-means et DBSCAN
clust1 <- paste("K-k", reskmeans$cluster, sep = "")
clust2 <- paste("Dbscan-k", res.db$cluster, sep = "")
table(clust1, clust2)

# Visualisation sous forme de diagramme d’alluvion pour la comparaison des clusters
Tab <- melt(table(clust1, clust2))
ggplot(Tab, aes(y = value, axis1 = clust1, axis2 = clust2)) +
  geom_alluvium(aes(fill = clust1)) +
  geom_stratum(width = 1 / 12) +
  geom_text(stat = "stratum", aes(label = after_stat(stratum))) +
  theme(legend.position = "none")

```

---
Les courbes obtenues comparent les résultats des algorithmes K-means et DBSCAN. Le tableau de comparaison montre les correspondances et divergences entre les clusters identifiés par les deux méthodes. Le diagramme visualise ces relations : des flux larges entre les clusters indiquent une forte correspondance, tandis que des petits flux correspondent à des divergences, comme des points considérés comme bruit par DBSCAN mais regroupés par K-means. 
---

# Clustering Hiérarchique (HAC)

```{r}
# Couper l'arbre à un certain nombre de clusters
k <- 2  # Par exemple, couper l'arbre pour obtenir 2 clusters
clusters <- cutree(hclust_res, k)

# Visualisation du dendrogramme avec les couleurs des clusters
plot(hclust_res, main = "Dendrogramme avec 2 clusters")
rect.hclust(hclust_res, k = k, border = "red")  # Ajouter des boîtes rouges autour des clusters

```
---
Le dendrogramme montre que les groupes qui se fusionnent tôt sont très similaires, tandis que ceux qui se rejoignent plus tard présentent des différences plus marquées. On observe que le nombre optimal de clusters, égal à 2, est cohérent avec les résultats obtenus par DBSCAN et K-means. Cela suggère que les trois méthodes identifient une structure similaire dans les données, avec une séparation nette entre deux groupes principaux.
---

```{r}
library(pheatmap)

# Créer une heatmap avec dendrogramme
pheatmap(Data[1:20, ], cluster_rows = TRUE, cluster_cols = TRUE, main = "Heatmap avec Dendrogramme sur les 20 premiers gènes")
```

---
La heatmap des 20 premiers gènes montre l'expression des gènes à travers différentes conditions, accompagnée de dendrogrammes qui regroupent les gènes et les conditions selon leurs profils d'expression. On remarque que les gènes du cluster en haut à droite, sont fortement corrélés avec les traitements 2 et 3. On peut en déduire que l'un des clusters regroupe principalement les gènes ayant subi les traitements 2 et 3, tandis que l'autre cluster correspond majoritairement aux gènes ayant subi le traitement 1.
---

## Nouvelle partie - Etude des différences entre les deux réplicats

---
Les lois de probabilité associés aux valeurs observées pour les deux réplicats sont elles significativement différentes ? Même question en se concentrant sur chaque traitement pris séparément. --> Réponses à partir du cours de janvier
---

---
Consigne : Etudier l'effet combiné du temps et du traitement sur la différence des réplicats à l'aide d'un modèle linéaire
---

```{r}
# Etape 1 : On effectue la différence des réplicats
p = ncol(Data)
#ncol(Data[,1:(p/2)])
#ncol(Data[,(p/2 + 1):p])
Data_Diff_R = Data[,1:(p/2)] - Data[,((p/2) + 1):p]
colnames(Data_Diff_R) <- sub("_[^_]*$", "", colnames(Data[,1:(p/2)]))
knitr::kable(Data_Diff_R)
```

```{r}
#Etape 2 : Création de Yij de telle sorte que Y_ij = Y_tsg (l'ordre est traitement, temps et gènes)
Y = as.vector(as.matrix(Data_Diff_R))
```

```{r}
#Etape 3 : Création des noms de lignes du vecteur Y
noms_Y <- outer(colnames(Data_Diff_R), rownames(Data_Diff_R), paste, sep="_")
noms_Y <- as.vector(t(noms_Y))

# Vérification des dimensions
length(noms_Y)
length(Y)

#Etape 4 : Création du vecteur traitement et temps
traitement <- sub("_.*", "", noms_Y)
temps <- as.numeric(sapply(strsplit(noms_Y, "[_h]"), function(x) x[2]))

# Etape 5 : Création du tableau pour le modèle linéaire
matrix = as.data.frame(cbind(Y, traitement, temps))
matrix$temps <- as.numeric(matrix$temps)
matrix$Y <- as.numeric(matrix$Y)


```

```{r}
rownames(matrix) <- noms_Y
knitr::kable(matrix)
```

```{r}
# Modèle ANCOVA à 2 facteur avec intéraction
ML <- lm(matrix$Y~matrix$traitement*matrix$temps, data = matrix)
summary(ML)
boxplot(matrix$Y~matrix$traitement*matrix$temps, data =matrix)
```

```{r}
# Modèle ANCOVA à 2 facteur sans intéractions
ML_sans_interaction = lm(matrix$Y~matrix$traitement+matrix$temps, data = matrix)
summary(ML_sans_interaction)
boxplot(matrix$Y~matrix$traitement+matrix$temps, data =matrix)
```

```{r}
# Courbes d'interactions
library(ggplot2)
library(gridExtra)

# Modèle sans interaction
M_non_interaction <- lm(Y ~ traitement + temps, data = matrix)

# Modèle avec interaction
M_interaction <- lm(Y ~ traitement * temps, data = matrix)

# Création des nouvelles données pour prédiction
new_data <- expand.grid(
  traitement = unique(matrix$traitement),
  temps = seq(min(matrix$temps), max(matrix$temps), length.out = 100)
)

# Prédictions
new_data$Y_non_interaction <- predict(M_non_interaction, newdata = new_data)
new_data$Y_interaction <- predict(M_interaction, newdata = new_data)

# Graphique pour le modèle sans interaction
plot_non_interaction <- ggplot(new_data, aes(x = temps, y = Y_non_interaction, color = traitement)) +
  geom_line(size = 1) +
  labs(title = "Modèle sans interaction",
       x = "Temps",
       y = "Valeurs prédites de Y",
       color = "Traitement") +
  theme_minimal()

# Graphique pour le modèle avec interaction
plot_interaction <- ggplot(new_data, aes(x = temps, y = Y_interaction, color = traitement)) +
  geom_line(size = 1) +
  labs(title = "Modèle avec interaction",
       x = "Temps",
       y = "Valeurs prédites de Y",
       color = "Traitement") +
  theme_minimal()

# Affichage côte à côte
grid.arrange(plot_non_interaction, plot_interaction, ncol = 2)
```
Dans le modèle sans interaction, les courbes sont parallèles, reflétant l'absence d'interdépendance entre le traitement et le temps. En revanche, dans le modèle avec interaction, les courbes montrent des variations de pente ou des croisements, indiquant une interaction significative entre ces deux variables. À partir de ces deux représentations, un test de sous-modèle est effectué pour déterminer si l'interaction doit être conservée dans le modèle final.

---
On commence à tester l'hypothèse de non-intéraction entre le facteur traitement et le variable temps.
---

```{r}
# Comparaison du modèle ANCOVA avec intéractions et sans intéractions
anova(ML, ML_sans_interaction)
```

---
La p-valeur observée est inférieur à 5% (1.426e-15 < 0.05). On rejette donc l'hypothèse de nullité des intéractions.
On conclut donc la présence d'intéraction dans le modèle; tester l'absence de facteur traitement et de variable temps n'a donc pas d'intéret, car toute variable constituant une interaction doit apparaître dans le modèle.
---


---
Consigne : Peut-on prévoir l’expression des gènes à 6h à partir de celle observée à 1h et du traitement
considéré ? Commenter la qualité de l’ajustement et la visualiser graphiquement.
---

```{r}
# extraire les Y à 1h et créer une matrice
p = ncol(Data)
Data_moy_R = (Data[,1:(p/2)] + Data[,((p/2) + 1):p])/2
Data_moy_R_1h = Data_moy_R[,c(1,7,13)]
Data_moy_R_6h = Data_moy_R[,c(6,12,18)]

colnames(Data_moy_R_1h) <- sub("_[^_]*$", "", colnames(Data_moy_R_1h))
#knitr::kable(Data_moy_R_1h)

colnames(Data_moy_R_6h) <- sub("_[^_]*$", "", colnames(Data_moy_R_6h))
#knitr::kable(Data_moy_R_6h)

Y_moy_1h = as.vector(as.matrix(Data_moy_R_1h))
Y_moy_6h = as.vector(as.matrix(Data_moy_R_6h))

#Etape 3 : Création des noms de lignes du vecteur Y
noms_Y <- outer(colnames(Data_moy_R_1h), rownames(Data_moy_R_1h), paste, sep="_")
noms_Y <- as.vector(t(noms_Y))

#Etape 4 : Création du vecteur traitement et temps
traitement <- sub("_.*", "", noms_Y)

# Etape 5 : Création du tableau pour le modèle linéaire
matrixMoy = as.data.frame(cbind(Y_moy_6h, Y_moy_1h, traitement))
matrixMoy$Y_moy_1h <- as.numeric(matrixMoy$Y_moy_1h)
matrixMoy$Y_moy_6h <- as.numeric(matrixMoy$Y_moy_6h)

knitr::kable(matrixMoy)
```


```{r}
ggplot(matrixMoy,aes(x=Y_moy_1h,y=Y_moy_6h))+ 
  geom_point(aes(shape=traitement,col=traitement)) 

ggplot(matrixMoy, aes(x = Y_moy_1h, y = Y_moy_6h, color = traitement)) +
  geom_point(aes(shape = traitement)) +
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE, linetype = "dashed") +
  geom_smooth(method = "lm", formula = y ~ x * traitement, se = FALSE) +
  labs(title = "Relation entre Y_moy_1h et Y_moy_6h",
       x = "Y_moy_1h",
       y = "Y_moy_6h")

```




```{r}
M_1h_to_6h <- lm(Y_moy_6h~Y_moy_1h*traitement, data = matrixMoy)
summary(M_1h_to_6h)
# tracer les courbes des valeurs predites vs les valeurs observées
# calculer les theta chapeau pour voir la difference (courbe residuals)
```


```{r}
#par(mfrow = c(2, 2))
plot(M_1h_to_6h)
```


```{r}
M_1h_to_6h_no_inter <- lm(Y_moy_6h ~ Y_moy_1h + traitement, data = matrixMoy)
anova(M_1h_to_6h_no_inter, M_1h_to_6h)
```
le test indique que l’ajout de l’interaction améliore significativement le modèle (p-valeur < 0.05), cela suggère que :
- L’effet de Y_1h sur Y_6h varie selon les traitements.
- L’interaction entre Y_1h et le traitement est pertinente.

### Conclusion
Le modèle ajusté montre une qualité d'ajustement modérée, avec un R2R2 de 27.06%, indiquant que 27% de la variance de Y6hY6h​ est expliquée par Y1hY1h​ et le traitement. Les termes d’interaction entre Y1hY1h​ et le traitement sont hautement significatifs (p<2e−16p<2e−16), ce qui montre que l’effet de Y1hY1h​ sur Y6hY6h​ varie selon le traitement. Les graphiques des résidus confirment que les hypothèses de linéarité et de normalité sont globalement respectées, bien que le modèle laisse une part importante de la variance inexpliquée. Cela suggère que Y1hY1h​ et les traitements contribuent à l’évolution temporelle de Y6hY6h​, mais d’autres facteurs pourraient être impliqués.

---
Consigne : Reprendre la question précédente en remplaçant 1h par 3h et comparer les résultats obtenus dans
les deux cas.
---
```{r}
p = ncol(Data)

Data_moy_R_3h = Data_moy_R[,c(3,9,15)]

colnames(Data_moy_R_3h) <- sub("_[^_]*$", "", colnames(Data_moy_R_3h))
#knitr::kable(Data_moy_R_3h)

Y_moy_3h = as.vector(as.matrix(Data_moy_R_3h))

#Etape 4 : Création du vecteur traitement et temps
traitement <- sub("_.*", "", noms_Y)

# Etape 5 : Création du tableau pour le modèle linéaire
matrixMoy2 = as.data.frame(cbind(Y_moy_6h, Y_moy_3h, traitement))
matrixMoy2$Y_moy_6h <- as.numeric(matrixMoy2$Y_moy_6h)
matrixMoy2$Y_moy_3h <- as.numeric(matrixMoy2$Y_moy_3h)

knitr::kable(matrixMoy2)
```


```{r}
ggplot(matrixMoy2, aes(x = Y_moy_3h, y = Y_moy_6h, color = traitement)) +
  geom_point(aes(shape = traitement)) +
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE, linetype = "dashed") +
  geom_smooth(method = "lm", formula = y ~ x * traitement, se = FALSE) +
  labs(title = "Relation entre Y_moy_6h et Y_moy_3h",
       x = "Y_moy_3h",
       y = "Y_moy_6h")
```


```{r}
M_3h_to_6h <- lm(Y_moy_6h~Y_moy_3h*traitement, data = matrixMoy2)
summary(M_3h_to_6h)
```
    Modèle 3h :
        Residual Standard Error (RSE) : 1.168
        R² ajusté : 0.770
        La variance expliquée est beaucoup plus élevée, indiquant que les prédictions du modèle pour les données à 3h sont nettement meilleures.

Interprétation :
Le modèle basé sur les données de 3h est nettement plus performant que celui de 1h, avec un R² ajusté presque trois fois supérieur et un RSE significativement plus faible. Cela signifie que les valeurs à 3h expliquent beaucoup mieux les valeurs à 6h.


```{r}
par(mfrow = c(2, 2))
plot(M_3h_to_6h)
```


```{r}
M_3h_to_6h_no_inter <- lm(Y_moy_6h ~ Y_moy_3h + traitement, data = matrixMoy2)
anova(M_3h_to_6h_no_inter, M_3h_to_6h)
```
Les résultats montrent que les données de 3h offrent un ajustement beaucoup plus précis que celles de 1h. Cela est confirmé par :
    - Une meilleure qualité de l'ajustement (RSE plus faible et R² ajusté plus élevé),
    - Une réduction de RSS plus importante dans l'ANOVA,
    - Une représentation graphique des résidus plus conforme aux hypothèses du modèle linéaire.
Ainsi, pour prédire Ymoy_6hYmoy_6h​, les données de 3h sont plus adaptées que celles de 1h.
